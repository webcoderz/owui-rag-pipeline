services:

  nvidia-rag-worker:
    build:
      context: ./docker/worker
      dockerfile: Dockerfile.worker
      args:
        HTTP_PROXY: ${HTTP_PROXY}
        HTTPS_PROXY: ${HTTPS_PROXY}
    environment:
      # Proxy support (optional)
      HTTP_PROXY: ${HTTP_PROXY}
      HTTPS_PROXY: ${HTTPS_PROXY}
      http_proxy: ${HTTP_PROXY}
      https_proxy: ${HTTPS_PROXY}

      # Required for NVIDIA RAG ingestion server internals (MinIO is used at import/startup time)
      # IMPORTANT: Do NOT use localhost here inside Docker. Use a resolvable hostname on the Docker network.
      MINIO_ENDPOINT: ${MINIO_ENDPOINT:-minio:9010}
      MINIO_ACCESSKEY: ${MINIO_ACCESSKEY:-minioadmin}
      MINIO_SECRETKEY: ${MINIO_SECRETKEY:-minioadmin}
      # Optional bucket name used by nv-ingest integrations (defaults inside the SDK if unset)
      NVINGEST_MINIO_BUCKET: ${NVINGEST_MINIO_BUCKET}

      # Milvus / vector DB (required for RAG generate and search). Do NOT use localhost inside Docker.
      VDB_ENDPOINT: ${VDB_ENDPOINT:-http://milvus:19530}
      # NVIDIA RAG SDK may read this at import; keep in sync with VDB_ENDPOINT.
      APP_VECTORSTORE_URL: ${VDB_ENDPOINT:-http://milvus:19530}

      # Optional: use your own OpenAI-compatible (vLLM) model for final generation instead of NVIDIA generation
      # - nvidia: uses NvidiaRAG.generate() (default)
      # - openai/vllm: uses NvidiaRAG.search() for retrieval, then calls OPENAI_BASE_URL with OPENAI_MODEL
      GENERATION_BACKEND: ${GENERATION_BACKEND:-nvidia}
      OPENAI_BASE_URL: ${OPENAI_BASE_URL:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-dummy}
      OPENAI_MODEL: ${OPENAI_MODEL:-}
    ports:
      - "8123:8123"
    depends_on:
      - owui-postgres 
    networks:
      - open-webui_default
      - nvidia-rag


  pipelines:
    image: ghcr.io/open-webui/pipelines:main
    environment:
      HTTP_PROXY: ${HTTP_PROXY}
      HTTPS_PROXY: ${HTTPS_PROXY}
      https_proxy: ${HTTPS_PROXY}
      OPENWEBUI_BASE_URL: http://open-webui:8080
      OPENWEBUI_API_KEY: ${OPENWEBUI_API_KEY}
      NVIDIA_WORKER_URL: http://nvidia-rag-worker:8123
      VDB_ENDPOINT: http://milvus:19530
      PIPELINES_API_KEY: 0p3n-w3bu!
      DATABASE_URL: postgresql://owui:owui@owui-postgres:5432/owui_bridge
      MAX_PARALLEL_FILE_INGEST: "4"
    volumes:
      - ./docker/pipelines/nvidia_ingest_bridge_pipe.py:/app/pipelines/nvidia_ingest_bridge_pipe.py:ro
      # Some Pipelines builds copy/import pipelines from a per-pipeline subfolder (created at startup).
      # Mount into both locations so updates always apply.
      - ./docker/pipelines/nvidia_ingest_bridge_pipe.py:/app/pipelines/nvidia_ingest_bridge_pipe/nvidia_ingest_bridge_pipe.py:ro

    depends_on:
      - owui-postgres
    networks:
      - open-webui_default
      - nvidia-rag

  owui-rag-tools:
    build:
      context: ./docker/tools
      dockerfile: Dockerfile.tools
    environment:
      DATABASE_URL: postgresql://owui:owui@owui-postgres:5432/owui_bridge
    ports:
      - "8181:8181"
    depends_on:
      - owui-postgres
    networks:
      - open-webui_default
      - nvidia-rag

  #caddy:
  #  build:
  #    context: ./docker/caddy
  #    dockerfile: Dockerfile
  #  image: owui-rag-caddy:local
  #  ports:
  #    - "${CADDY_HTTP_PORT:-80}:80"
  #  volumes:
  #    - ./docker/caddy/Caddyfile:/etc/caddy/Caddyfile:ro
  #  networks:
  #    - open-webui_default
  #    - nvidia-rag

  owui-postgres:
    image: postgres:16
    environment:
      POSTGRES_DB: owui_bridge
      POSTGRES_USER: owui
      POSTGRES_PASSWORD: owui
    volumes:
      - owui_pg:/var/lib/postgresql/data
    networks:
      - open-webui_default

volumes:
  owui_pg:
  pipelines_data: 
  #open-webui:

networks:
  open-webui_default:
    external: true
  nvidia-rag:
    external: true