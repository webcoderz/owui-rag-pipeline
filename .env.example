# Open WebUI (pipelines + optional backfill script)
OPENWEBUI_BASE_URL=http://open-webui:8080
OPENWEBUI_API_KEY=your-openwebui-service-token

# Proxy (optional). Pipelines/build use these. Worker has proxy cleared in compose so it reaches Milvus/OPENAI_BASE_URL direct. If you pass proxy to the worker, set no_proxy so Milvus and the LLM host bypass it (e.g. no_proxy=milvus-standalone,<OPENAI host or IP>).
HTTP_PROXY=
HTTPS_PROXY=
NO_PROXY=*

# Milvus / vector DB (worker and pipelines). Use the Docker hostname of your Milvus service, not localhost.
# If your stack runs Milvus as "milvus-standalone", set: VDB_ENDPOINT=http://milvus-standalone:19530
# Worker also sets APP_VECTORSTORE_URL from this (NVIDIA SDK may use it at init).
VDB_ENDPOINT=http://milvus:19530

# Backfill script (scripts/backfill_owui_knowledge_to_milvus.py): set when running from host.
# NVIDIA_WORKER_URL=http://localhost:8123
# COLLECTION_PREFIX=owui

# NVIDIA RAG SDK (MinIO; use Docker hostname, not localhost)
MINIO_ENDPOINT=minio:9010
MINIO_ACCESSKEY=minioadmin
MINIO_SECRETKEY=minioadmin
# SDK requires MINIO_BUCKET (use a bucket that exists in MinIO; create nv-ingest or use default-bucket)
MINIO_BUCKET=nv-ingest
NVINGEST_MINIO_BUCKET=nv-ingest

# Optional: OpenAI/vLLM for generation instead of NVIDIA (e.g. gpt-oss on vLLM)
# OPENAI_BASE_URL = host:port only; worker appends /v1/chat/completions (e.g. http://vllm-host:8000)
# OPENAI_MODEL = exact model id the endpoint expects: for Open WebUI API use UI model id (e.g. openai/gpt-oss-120b); for vLLM/direct use raw name (e.g. gpt-oss-120b)
GENERATION_BACKEND=nvidia
OPENAI_BASE_URL=
OPENAI_MODEL=
OPENAI_API_KEY=dummy

# Caddy proxy: host port (default 80 for networks that only allow 80/443). Use 443 for HTTPS.
CADDY_HTTP_PORT=80

# Optional: pipeline debug log path (default /app/pipeline_debug/debug.log with volume mount). File-only, no network; copy contents to share.
# DEBUG_LOG_PATH=/app/pipeline_debug/debug.log
