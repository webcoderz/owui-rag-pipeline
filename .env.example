# Open WebUI (pipelines + optional backfill script)
OPENWEBUI_BASE_URL=http://open-webui:8080
# Optional. Pipeline can use either (1) OPENWEBUI_API_KEY or (2) the request's Bearer token (when Open WebUI
# forwards Authorization to Pipelines). For user/group-scoped collections, omit the key and rely on the request token.
# OPENWEBUI_API_KEY=your-openwebui-service-token
# When using API key: set ENABLE_FORWARD_USER_INFO_HEADERS=True in Open WebUI so user headers are sent.
# See README "Ensuring user info is forwarded".

# Proxy (optional). Pipelines/build use these. Worker has proxy cleared in compose so it reaches Milvus/OPENAI_BASE_URL direct. If you pass proxy to the worker, set no_proxy so Milvus and the LLM host bypass it (e.g. no_proxy=milvus-standalone,<OPENAI host or IP>).
HTTP_PROXY=
HTTPS_PROXY=
NO_PROXY=*

# Milvus / vector DB (worker and pipelines). Use the Docker hostname of your Milvus service, not localhost.
# If your stack runs Milvus as "milvus-standalone", set: VDB_ENDPOINT=http://milvus-standalone:19530
# Worker also sets APP_VECTORSTORE_URL from this (NVIDIA SDK may use it at init).
VDB_ENDPOINT=http://milvus:19530

# Pipelines: collection naming. Prefix for Milvus collection names (default owui).
# COLLECTION_PREFIX=owui
# Set to true to give each /ingest library a unique collection (e.g. owui-u-anon-library-a1b2c3d4) instead of one shared per user.
# UNIQUE_LIBRARY_COLLECTION_PER_INGEST=false
# When using a service account/API key, Open WebUI often sends no user â†’ collection names use "anon". Set this to a stable name (e.g. service-acct) to get owui-u-<this>-library instead of owui-u-anon-library.
# COLLECTION_USER_KEY=

# Backfill script (scripts/backfill_owui_knowledge_to_milvus.py): set when running from host.
# NVIDIA_WORKER_URL=http://localhost:8123

# NVIDIA RAG SDK (MinIO; use Docker hostname, not localhost)
MINIO_ENDPOINT=minio:9010
MINIO_ACCESSKEY=minioadmin
MINIO_SECRETKEY=minioadmin
# SDK requires MINIO_BUCKET (use a bucket that exists in MinIO; create nv-ingest or use default-bucket)
MINIO_BUCKET=nv-ingest
NVINGEST_MINIO_BUCKET=nv-ingest

# Optional: OpenAI/vLLM for generation instead of NVIDIA (e.g. gpt-oss on vLLM)
# OPENAI_BASE_URL = host:port only; worker appends /v1/chat/completions (e.g. http://vllm-host:8000)
# OPENAI_MODEL = exact model id the endpoint expects: for Open WebUI API use UI model id (e.g. openai/gpt-oss-120b); for vLLM/direct use raw name (e.g. gpt-oss-120b)
GENERATION_BACKEND=nvidia
OPENAI_BASE_URL=
OPENAI_MODEL=
OPENAI_API_KEY=dummy

# Caddy proxy: host port (default 80 for networks that only allow 80/443). Use 443 for HTTPS.
CADDY_HTTP_PORT=80

# Optional: pipeline debug. PIPE_DEBUG=true logs request shape and whether user-info headers (X-OpenWebUI-User-Id, X-OpenWebUI-User-Email) are present (no values logged).
# PIPE_DEBUG=false
# Optional: pipeline debug log path (default /app/pipeline_debug/debug.log with volume mount). File-only, no network; copy contents to share.
# DEBUG_LOG_PATH=/app/pipeline_debug/debug.log
